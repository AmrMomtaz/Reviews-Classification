{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmrMomtaz/Reviews-Classification/blob/main/Bert_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEnKvtPajI_r"
      },
      "source": [
        "# **Movies reviews classification using BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_j45zirjvL7"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ9_5bBH1wsq"
      },
      "source": [
        "IMDB is the most globally famous movie reviews website where you can publish a review for\n",
        "any film you watched. Classifying the positive reviews and the negative ones can be useful for\n",
        "several purposes such as giving an overall rating for the film or making statistical analysis about\n",
        "the preferences of people from different countries, age levels, etc... So IMDB dataset is released\n",
        "which composed of 50k reviews labeled as positive or negative to enable training movie reviews\n",
        "classifiers. Moreover, NLP tasks are currently solved based on pretrained language models such\n",
        "as BERT. These models provide a deep understanding of both semantic and contextual aspects\n",
        "of language words, sentences or even large paragraphs due to their training on huge corpus for\n",
        "very long time. In this notebook We will download the IMDB dataset from kaggle using this <a href = \"https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\">\n",
        "Link</a>. Then, we will train BERT based classifier for movie reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to use split the data 70% training and 10% dev and 20% for testing.\n",
        "We will use BERT pretrained weights first and we will and we will use early stopping using 3 steps"
      ],
      "metadata": {
        "id": "Yr6JKicPUXFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9mccWalCjQY"
      },
      "source": [
        "## Installation\n",
        "\n",
        "In this section we are going to get the problem data and install required libraries.\n",
        "First you have to **upload** the kaggle API token to the current running sesion then run the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4CcD9V9pKqW",
        "outputId": "64150796-9b5a-4957-d0a1-794db254bc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 39% 10.0M/25.7M [00:00<00:00, 104MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 126MB/s]\n",
            "Archive:  /content/imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 19.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "# Cloning Data \n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "! unzip /content/imdb-dataset-of-50k-movie-reviews.zip\n",
        "! rm /content/imdb-dataset-of-50k-movie-reviews.zip\n",
        "! pip install transformers\n",
        "\n",
        "# Importing libraries\n",
        "# For EDA and data cleaning\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "# For preprocessing the data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "# For creating the dataset and the bert model\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWhXsp7qELCZ"
      },
      "source": [
        "## EDA and cleaning the data\n",
        "In this section we are going to do some EDA for better insight of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "3Iz49mohExkL",
        "outputId": "f3e29f38-0a7d-4408-bd12-1b2a75641006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data frame shape = (50000, 2)\n",
            "Dataframe description: \n",
            "                                                   review sentiment\n",
            "count                                               50000     50000\n",
            "unique                                              49582         2\n",
            "top     Loved today's show!!! It was a variety and not...  positive\n",
            "freq                                                    5     25000\n",
            "Dataframe duplicated rows: 418\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7cbcf383-f775-43fb-b90e-6900f3a92317\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7cbcf383-f775-43fb-b90e-6900f3a92317')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7cbcf383-f775-43fb-b90e-6900f3a92317 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7cbcf383-f775-43fb-b90e-6900f3a92317');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Creating dataframe and changing the lables to categorical\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "print(f\"Data frame shape = {df.shape}\")\n",
        "print(f\"Dataframe description: \\n{df.describe()}\")\n",
        "print(f\"Dataframe duplicated rows: {sum(df.duplicated())}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will drop the duplicated rows and we will see the number of positive and negative examples to see if under-sampling or over-sampling is required and we will change the categorical column (\"sentiment\") to zeros and ones and change its name to label"
      ],
      "metadata": {
        "id": "BcSY7i8J7spz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "F94WY2iCIIkl",
        "outputId": "aa6c5154-2842-404c-d5ad-a00ff3a6f8cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Droping the duplicated rows\n",
        "df.drop_duplicates(inplace = True)\n",
        "# Plotting the sentiment column\n",
        "sb.countplot(x='sentiment', data=df)\n",
        "# Modifying the sentiment column\n",
        "df[\"sentiment\"] = df[\"sentiment\"].astype('category')\n",
        "df[\"label\"] = df[\"sentiment\"].cat.codes\n",
        "df = df.drop(columns=['sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there is no need for under sampling or over sampling since the the data is balanced for both classes"
      ],
      "metadata": {
        "id": "bmoN2PhWRwLv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT-UIuZOcLNC"
      },
      "source": [
        "## Data Pre-processing and splitting\n",
        "\n",
        "Run the following cell to pre-process the data. In the data pre-processing there are four things to be done:\n",
        "* Remove punctuations\n",
        "* Remove stop words\n",
        "* Lower case all words\n",
        "* Lemmatization of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVTLeCWqFT7X"
      },
      "outputs": [],
      "source": [
        "# Lowercase all characters\n",
        "df[\"review\"] = df.review.apply(lambda x : str.lower(x))\n",
        "\n",
        "# Removing punctiations and <br />\n",
        "df[\"review\"] = df.review.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
        "df[\"review\"] = df.review.apply(lambda x : x.replace('br', ''))\n",
        "\n",
        "# Removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df[\"review\"] = df.review.apply(\n",
        "    lambda x : ' '.join(word for word in x.split() if word not in stop_words))\n",
        "df[\"review\"] = df.review.apply(lambda x : x.replace('   ', ' '))\n",
        "\n",
        "# Lemmatization\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "df[\"review\"] = df.review.apply(lambda x : lemmatize_text(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will dplit the data to 70% train ,10% dev and 20% test and we will stratify and resample it to be randomly ordered"
      ],
      "metadata": {
        "id": "jpc6OmplTzCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evP_qi9vY87M"
      },
      "outputs": [],
      "source": [
        "# Stratify and split\n",
        "train_list = []\n",
        "test_list = []\n",
        "val_list = []\n",
        "grouped_df = df.groupby('labels')\n",
        "\n",
        "for i, g in grouped_df:\n",
        "    train, val, test = np.split(g, [int(.7 * len(g)), int(.8 * len(g))])\n",
        "    train_list.append(train); test_list.append(test); val_list.append(val)\n",
        "\n",
        "df_train = pd.concat(train_list)\n",
        "df_val = pd.concat(val_list)\n",
        "df_test = pd.concat(test_list)\n",
        "# Resampling\n",
        "df_train = df_train.sample(frac = 1)\n",
        "df_val = df_val.sample(frac = 1)\n",
        "df_test = df_test.sample(frac = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7tykwVHuu_6"
      },
      "source": [
        "## Building Dataset and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYzPXoDdurja"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = list(df.labels)\n",
        "        self.texts = list(df.review.apply(lambda x :tokenizer(x, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\")))\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpgdUls4xpgS"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout_bert = nn.Dropout(0.5)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear1 = nn.Linear(768, 512)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
        "        self.linear3 = nn.Linear(256, 128)\n",
        "        self.linear4 = nn.Linear(128, 64)\n",
        "        self.linear_out = nn.Linear(64, 1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, x = self.bert(input_ids=input_id, attention_mask=mask,return_dict=False)\n",
        "        x = self.dropout_bert(x)\n",
        "        x = self.relu1(self.linear1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu2(self.linear2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu3(self.linear3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu4(self.linear4(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.linear_out(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing the model"
      ],
      "metadata": {
        "id": "FGy_tiDOXAxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKxRlxXxt_fi",
        "outputId": "8fe0e0e2-e297-4d04-f197-02df5bbcd431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 32/32 [00:38<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.021                 | Train Accuracy:  0.818                 | Val Loss:  0.322                 | Val Accuracy:  0.840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:38<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.020                 | Train Accuracy:  0.846                 | Val Loss:  0.317                 | Val Accuracy:  0.865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:38<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.020                 | Train Accuracy:  0.905                 | Val Loss:  0.309                 | Val Accuracy:  0.935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:38<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.019                 | Train Accuracy:  0.925                 | Val Loss:  0.306                 | Val Accuracy:  0.935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:38<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.019                 | Train Accuracy:  0.965                 | Val Loss:  0.298                 | Val Accuracy:  0.955\n"
          ]
        }
      ],
      "source": [
        "train_loss = list()\n",
        "val_loss = list()\n",
        "train_accuracy = list()\n",
        "val_accuracy = list()\n",
        "\n",
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate, weight_decay=1e-5)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output.squeeze(1), train_label.float())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                y_pred_tag = torch.round(output.squeeze(1))\n",
        "                acc = (y_pred_tag == train_label).sum()\n",
        "\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output.squeeze(1), val_label.float())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    y_pred_tag = torch.round(output.squeeze(1))\n",
        "                    acc = (y_pred_tag == val_label).sum()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            train_loss.append(total_loss_train / len(train_data))\n",
        "            train_accuracy.append(total_acc_train / len(train_data))\n",
        "            val_loss.append(total_loss_val / len(val_data))\n",
        "            val_accuracy.append(total_acc_val / len(val_data))\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "            \n",
        "            \n",
        "                  \n",
        "EPOCHS = 5\n",
        "model = BertClassifier()\n",
        "LR = 1e-5\n",
        "\n",
        "train(model, df_train, df_val, LR, EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the model train and validation loss and accuracy"
      ],
      "metadata": {
        "id": "Pp_11UZqXJ1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = [18, 5])\n",
        "plt.suptitle(\"Loss and Accuracy in training\")\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epoch_list,train_loss);\n",
        "plt.plot(epoch_list,val_loss);\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\",\"Validation\"])\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epoch_list,train_accuracy);\n",
        "plt.plot(epoch_list,val_accuracy);\n",
        "plt.xlabel(\"Epochs\");\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Train\",\"Validation\"]);"
      ],
      "metadata": {
        "id": "jjnIydXw4wjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model"
      ],
      "metadata": {
        "id": "AvdZ84NoXRBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=32)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    final_confusion_matrix = np.zeros([2,2])\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "              output = model(input_id, mask)\n",
        "              y_pred_tag = torch.round(output.squeeze(1))\n",
        "              acc = (y_pred_tag == test_label).sum()\n",
        "              total_acc_test += acc\n",
        "              final_confusion_matrix += confusion_matrix(test_label.cpu().data.numpy(), y_pred_tag.cpu().data.numpy())\n",
        "    \n",
        "    TP = final_confusion_matrix[0][0]\n",
        "    FP = final_confusion_matrix[0][1]\n",
        "    FN = final_confusion_matrix[1][0]\n",
        "    TN = final_confusion_matrix[1][1]\n",
        "    Percision = TP/(TP+FP)\n",
        "    Recall = TP/(TP+FN)\n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    print(f'Percision : {Percision}')\n",
        "    print(f'Sensitivity (Recall) : {Recall}')\n",
        "    print(f'Specifity : {TN/(TN+FP)}')\n",
        "    print(f'F1 Score = {((2*Percision*Recall)/(Percision+Recall))}')\n",
        "    print('Confusion Matrix :')\n",
        "    print(final_confusion_matrix)\n",
        "    \n",
        "evaluate(model, df_test)"
      ],
      "metadata": {
        "id": "3gW3sJXp40i2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bert-Classifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}